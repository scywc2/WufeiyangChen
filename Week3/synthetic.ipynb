{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "033747e8-8f0e-43e2-846b-ff2e9e5329d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data/real\", exist_ok=True)         \n",
    "os.makedirs(\"data/synthetic/images\", exist_ok=True) \n",
    "os.makedirs(\"data/synthetic/depth\", exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d598e84d-706c-4d2a-a93a-f2503addbd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class MixedDomainDataset(Dataset):\n",
    "    def __init__(self, real_dir, synthetic_dir):\n",
    "        self.real_images = sorted([os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith('.jpg')])\n",
    "        self.synthetic_images = sorted([os.path.join(synthetic_dir, 'images', f) for f in os.listdir(os.path.join(synthetic_dir, 'images'))])\n",
    "        self.synthetic_depths = sorted([os.path.join(synthetic_dir, 'depth', f.replace('.jpg', '.npy')) for f in os.listdir(os.path.join(synthetic_dir, 'images'))])\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 512)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.real_images) + len(self.synthetic_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.real_images):\n",
    "            img = Image.open(self.real_images[idx]).convert('RGB')\n",
    "            img = self.transform(img)\n",
    "            return {'image': img, 'is_real': True, 'filepath': self.real_images[idx]}\n",
    "        else:\n",
    "            syn_idx = idx - len(self.real_images)\n",
    "            img = Image.open(self.synthetic_images[syn_idx]).convert('RGB')\n",
    "            depth = np.load(self.synthetic_depths[syn_idx])\n",
    "            img = self.transform(img)\n",
    "            depth = torch.from_numpy(depth).unsqueeze(0).float()\n",
    "            return {'image': img, 'depth': depth, 'is_real': False, 'filepath': self.synthetic_images[syn_idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038f8db0-2b43-44dd-bcd1-74781dae20ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# dataset = MixedDomainDataset('data/real', 'data/synthetic')\n",
    "\n",
    "# def plot_sample(sample):\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.imshow(sample['image'].permute(1, 2, 0).numpy() * 0.5 + 0.5) \n",
    "#     plt.title(f\"{'Real' if sample['is_real'] else 'Synthetic'}: {os.path.basename(sample['filepath'])}\")\n",
    "    \n",
    "#     if not sample['is_real']:\n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.imshow(sample['depth'].squeeze(), cmap='jet')\n",
    "#         plt.title('Depth Map')\n",
    "        \n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.hist(sample['depth'].flatten().numpy(), bins=50)\n",
    "#         plt.title('Depth Distribution')\n",
    "#     plt.show()\n",
    "\n",
    "# plot_sample(dataset[0])\n",
    "# plot_sample(dataset[len(dataset.real_images)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a52d06ec-7751-421e-8d09-3e16517aaddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "class GradientReversal(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "def grad_reverse(x, alpha=1.0):\n",
    "    return GradientReversal.apply(x, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e732644a-1422-4934-b5c3-4c97e8309288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DomainClassifier(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=5, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2)\n",
    "        self.fc = nn.Linear(128, 2)\n",
    "        \n",
    "    def forward(self, x, alpha=1.0):\n",
    "        x = grad_reverse(x, alpha)\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.MaxPool2d(2)(x)\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = nn.AdaptiveAvgPool2d(1)(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7108aa04-404c-4860-b53b-39c5fb6528b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthDomainAdaptationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            *list(torch.hub.load('pytorch/vision', 'resnet18', pretrained=True).children())[:-2]\n",
    "        )\n",
    "        \n",
    "        self.depth_head = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.domain_classifier = DomainClassifier(512)\n",
    "    \n",
    "    def forward(self, x, alpha=1.0):\n",
    "        features = self.feature_extractor(x)\n",
    "        depth = self.depth_head(features)\n",
    "        domain_logits = self.domain_classifier(features, alpha)\n",
    "        return depth, domain_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de299603-4452-4665-9e5a-9418d5e0244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm.tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images = batch['image'].to(device)\n",
    "        is_real = batch['is_real'].to(device)\n",
    "        \n",
    "        depths = torch.zeros(len(images), 1, 64, 128).to(device) \n",
    "        if not all(is_real):\n",
    "            synth_mask = ~is_real\n",
    "            depths[synth_mask] = batch['depth'][synth_mask].to(device)\n",
    "        \n",
    "        pred_depths, domain_logits = model(images, alpha=2.0)\n",
    "        \n",
    "        depth_loss = nn.MSELoss()(pred_depths[~is_real], depths[~is_real])\n",
    "        domain_labels = is_real.long()  # 0=real, 1=synthetic\n",
    "        domain_loss = nn.CrossEntropyLoss()(domain_logits, domain_labels)\n",
    "        loss = depth_loss + 0.1 * domain_loss \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5c4c07-6f12-4d55-960a-ba6a3eb8b79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "\n",
    "def visualize_features():\n",
    "    model.eval()\n",
    "    real_features = []\n",
    "    synth_features = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in DataLoader(dataset, batch_size=32):\n",
    "            images = batch['image'].to(device)\n",
    "            features = model.feature_extractor(images)\n",
    "            features = features.mean(dim=[2,3]).cpu().numpy()\n",
    "            \n",
    "            for i in range(len(batch['is_real'])):\n",
    "                if batch['is_real'][i]:\n",
    "                    real_features.append(features[i])\n",
    "                else:\n",
    "                    synth_features.append(features[i])\n",
    "    \n",
    "    all_features = np.vstack([real_features, synth_features])\n",
    "    tsne = TSNE(n_components=2)\n",
    "    reduced = tsne.fit_transform(all_features)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(reduced[:len(real_features), 0], reduced[:len(real_features), 1], c='r', label='Real')\n",
    "    plt.scatter(reduced[len(real_features):, 0], reduced[len(real_features):, 1], c='b', label='Synthetic')\n",
    "    plt.title(\"Feature Space Distribution (t-SNE)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "visualize_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471ff78a-1881-4567-a47f-244210e6620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_depth_prediction():\n",
    "    model.eval()\n",
    "    test_sample = dataset[len(dataset.real_images)] \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_depth, _ = model(test_sample['image'].unsqueeze(0).to(device))\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(test_sample['image'].permute(1, 2, 0).numpy() * 0.5 + 0.5)\n",
    "    plt.title(\"Input Image\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(test_sample['depth'].squeeze(), cmap='jet')\n",
    "    plt.title(\"Ground Truth Depth\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pred_depth.squeeze().cpu().numpy(), cmap='jet')\n",
    "    plt.title(\"Predicted Depth\")\n",
    "    plt.show()\n",
    "\n",
    "test_depth_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbb8d0-17d8-4593-ae35-79a972e1f47c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
